# WP lld proposal 
Example of app with some style and way how to work with provided technologies <br>
![hhd.png](static%2Fhhd.png)

## How to start it (Will try to dockerize wp app for testing)
In the first of all build the entire project (`mvn clean install -DskipTests -Dmaven.build.cache.enabled=false`) <br> 
Execute the following instructions under `app/target/classes/deployment`
### [NOT SUPPORTED NOW] Using Openshift Local (configured with 1 replicas and partitions, very unstable on Apple Silicon and with <16 ram)
1. Code Ready Containers configuration (crc, steps for configuration via terminal, you can go with podman, but terminal is more stable)
   - install https://developers.redhat.com/products/openshift-local/overview
   - execute `crc setup` to obtain suitable bundle (in case of any trouble with connection, go with background process `crc daemon` and try again)
   - download secrets from your https://console.redhat.com/openshift/create/local (in .json format)
   - provide details about secret: `crc config set pull-secret-file <path/to/secret.json>`
   - try to run `crc start -p <path/to/secret> --log-level debug` (debug and secret are provided for better troubleshooting, is very tricky to setup openhsift locally)
   - wait for crc starting, can be checked via `crc status`
   - in case of any issue, try `crc stop`, `crc start -p <path/to/secret> --log-level debug`, also you can try to stop, `crc delete` and start
2. Openshift (oc) configuration
   - after successfully starting crc, use `crc oc-env` (to have predefined oc route) and `eval $(crc oc-env)`, register in current terminal session `oc` command.
   - after starting crc will be generated kubernetes address, can be found `crc console --credentials`, use this to login in oc `oc login -u kubeadmin <kubernetes endpoint>`
   - create new project `oc new-project kafka-example` (in scripts are used kafka-example naming, if you want different, should be changed in `app/target/classes/deployment/openshift`)
   - add to project new operator: `oc apply -f 'https://strimzi.io/install/latest?namespace=kafka-example'` (operator - solution providing utility for working with kafka in kubernetes, can be replaced by pure containers, but for example is ok :D )
   - go to `app/target/classes/deployment/openshift` and feel free to execute `startup.sh`
   - wait for `All resources have been deployed and are ready.`
3. Find connections:
   - for broker boostrap service use: `oc get kafkas/kafka-cluster -o=jsonpath={.status.listener}`, there will be able to find `boostrapServer: "localhost:<randomPort>`, are configured NodePort for this 
   - for ui, use `oc get routes kafka-ui`, and copy see the host
   - for schema, use `oc get routes schema-registry`, and copy see the host
4. Troubleshooting commands:
   - `oc get pods` -> will show all pods and their readiness (we should have 3-3 zookeper-kafka, strimzi and kafka operators, kafka-ui and schema-registry)
   - `oc get kafkas` -> will show all clusters predefines in kafka directory on oc vm (basically our `kafka-cluster` state)
   - `crc status` -> status of crc (can become unreachable)
   - `crc console` -> access Openshift GUI, use kubeadmin, obtain credentials `crc console --credentials`
   - add to path your kubernetes config from cache (PATH:KUBECONFIG=`/.crc/cache/<crc_bundle_vfkit>/kubeadmin`)
   - in case that crc is failing after specific time, try to increase ram using `crc config set memory <at least 10G, it should be by default 10G, but better to use 16 if is possible>`
5. For running app, use `mvn spring-boot:run`

### Using Podman (light version, 1 replica and partition)
1. Use podman engine (at least 2 cpu and 4 ram)
2. Go `app/target/classes/deployment/docker`
3. Use `startup.sh` and `cleanup.sh` (check if sh scripts have privileges for execution and double check, if used commands are supported in your OS)
4. Default values, provided in `application.yml` are configured based on this deployment (`localhost:8090`-kafka-ui, `localhost:9021` - confluent-control-center, similar things :D)
5. For running app
   - use `mvn -pl app spring-boot:run -Dspring-boot.run.jvmArguments="-DKAFKA_BOOSTRAP_SERVER_HOST=<targetHost> -DSCHEMA_REGISTRY_HOST=<targetHost> -DDB_HOST=localhost:5432 -DDB_USERNAME=postgres -DDB_PASS=postgres"`

### AWS (TBU, will add my own credentials for testing purposes. OVERPRICED !!!!!!!!)

### Proposed best practices
## Main ideas
1. Using Contract based inter-component communication (in example is used AVRO, but can be used JSON or Protobuf)
   - Actually representing DTO objects, which will be generated via maven plugins (ready solutions, which are able to be fully customized. PS. I HOPE :D )
2. Go with domain based layering, which will be split in kind of Presentation layer (api, consumer, producer, delegate, adapter), Business Layer (service and Validator) and Persistence layer (dao)
3. Only and Only presentation layer knows about third party Models (for example generated by AVRO Pojos/DTOs)
4. Services isolation. Single service - representing specific business need. Common reusable functionality (at least in 2-3 services) should be extracted in some lib. Service know about lib, but don't about another service.
5. Try to define from start main dependencies for the future usage (for example choose asserting library (more about development)). Can we go with BOM for our projects ? 
6. Standardise best practices and try to cover this by ArchUnit
... should be adjusted 

## Questions:
1. TestContainers ? Will we use KafkaTests from spring and 
2. UnitTests, IntegrationTest and Component Tests ?
3. Spotless ? 
4. TEST UTILS ?